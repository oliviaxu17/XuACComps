---
header-includes:
- \usepackage{graphicx,latexsym}
- \usepackage{amssymb,amsthm,amsmath}
- \usepackage{longtable,booktabs,setspace}
output:
  pdf_document:
    fig_height: 3
    fig_width: 5
    latex_engine: xelatex
  html_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---

<!--
You can delete the header-includes (lines 3-6 above) if you like and also the chunk below since it is loaded in the skeleton.Rmd file.  They are included so that chap3.Rmd will compile by itself when you hit Knit PDF.
-->

```{r include_packages_2, include = FALSE}
# This chunk ensures that the reedtemplates package is
# installed and loaded. This reedtemplates package includes
# the template files for the thesis and also two functions
# used for labeling and referencing
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(dplyr))
    install.packages("dplyr", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("ggplot2", repos = "http://cran.rstudio.com")
if(!require(reedtemplates)){
  library(devtools)
  devtools::install_github("ismayc/reedtemplates")
  }
library(reedtemplates)
```


# Latent Dirichlet Allocation: An Expository Review {#ref_labels}

### What is LDA?
LDA is a statistical model used to detect underlying groups that would explain why some parts of the data look similar.  It is commonly used to identify patterns that are not easily interpretable.  In this chapter, I will be outlining how I applied the LDA method on Twain's works.  As a resource and for inspiration, I used the "Data Until I Die!" blog post on topic modeling called "A Rather Nosy Topic Model Analysis of the Enron Email Corpus".

### Twain and LDA
Because LDA is used to find patterns and meaning in a large amount of documents, I needed to break up my Twain documents in a logical fashion so that I could have more than six documents (from the original six books).  I decided to break each book up by chapters, which seemed like the best way to break the stories up into smaller increments but still have them make sense and be complete in themselves.  I ended up with somewhere close to 300 chapters after I run my code.  

The first thing the blog did was to create a string with stop words.  A stop word is a word that we're not interested in looking at, so it gets thrown out in the process of finding the natural groupings.  Using the blog post's stop words list as an inspiration, I added my own words to the list.  I kept going back and forth between the output and the stop words list to add words that I didn't want in the output (mostly every day words that didn't tell me much about the topics). 

```{r, include = FALSE}
library(stringr)
library(plyr)
library(tm)
library(tm.plugin.mail)
library(SnowballC)
library(topicmodels)
```

```{r, include = FALSE}
twaincorpus <- Corpus(DirSource("/home/class17/sxu17/XuACComps/TwainTexts"))

extendedstopwords=c("a","about","above","across","after","MIME Version","forwarded","again","against","all","almost","alone","along","already","also","although","always","am","among","an","and","another","any","anybody","anyone","anything","anywhere","are","area","areas","aren't","around","as","ask","asked","asking","asks","at","away","b","back","backed","backing","backs","be","became","because","become","becomes","been","before","began","behind","being","beings","below","best","better","between","big","both","but","by","c","came","can","cannot","can't","case","cases","certain","certainly","clear","clearly","come","could","couldn't","d","did","didn't","differ","different","differently","do","does","doesn't","doing","done","don't","down","downed","downing","downs","during","e","each","early","either","end","ended","ending","ends","enough","even","evenly","ever","every","everybody","everyone","everything","everywhere","f","face","faces","fact","facts","far","felt","few","find","finds","first","for","four","from","full","fully","further","furthered","furthering","furthers","g","gave","general","generally","get","gets","give","given","gives","go","going","good","goods","got","great","greater","greatest","group","grouped","grouping","groups","h","had","hadn't","has","hasn't","have","haven't","having","he","he'd","he'll","her","here","here's","hers","herself","he's","high","higher","highest","him","himself","his","how","however","how's","i","i'd","if","i'll","i'm","important","in","interest","interested","interesting","interests","into","is","isn't","it","its","it's","itself","i've","j","just","k","keep","keeps","kind","knew","know","known","knows","l","large","largely","last","later","latest","least","less","let","lets","let's","like","likely","long","longer","longest","m","made","make","making","man","many","may","me","member","members","men","might","more","most","mostly","mr","mrs","much","must","mustn't","my","myself","n","necessary","need","needed","needing","needs","never","new","newer","newest","next","no","nobody","non","noone","nor","not","nothing","now","nowhere","number","numbers","o","of","off","often","old","older","oldest","on","once","one","only","open","opened","opening","opens","or","order","ordered","ordering","orders","other","others","ought","our","ours","ourselves","out","over","own","p","part","parted","parting","parts","per","perhaps","place","places","point","pointed","pointing","points","possible","present","presented","presenting","presents","problem","problems","put","puts","q","quite","r","rather","really","right","room","rooms","s","said","same","saw","say","says","second","seconds","see","seem","seemed","seeming","seems","sees","several","shall","shan't","she","she'd","she'll","she's","should","shouldn't","show","showed","showing","shows","side","sides","since","small","smaller","smallest","so","some","somebody","someone","something","somewhere","state","states","still","such","sure","t","take","taken","than","that","that's","the","their","theirs","them","themselves","then","there","therefore","there's","these","they","they'd","they'll","they're","they've","thing","things","think","thinks","this","those","though","thought","thoughts","three","through","thus","to","today","together","too","took","toward","turn","turned","turning","turns","two","u","under","until","up","upon","us","use","used","uses","v","very","w","want","wanted","wanting","wants","was","wasn't","way","ways","we","we'd","well","we'll","wells","went","were","we're","weren't","we've","what","what's","when","when's","where","where's","whether","which","while","who","whole","whom","who's","whose","why","why's","will","with","within","without","won't","work","worked","working","works","would","wouldn't","x","y","year","years","yes","yet","you","you'd","you'll","young","younger","youngest","your","you're","yours","yourself","yourselves","you've","z", "littl", "tom", "warnt", "hundr", "look", "nigger", "aint", "dont", "peopl", "chapter", "wilson", "didnt", "wouldnt", "couldnt", "peopl", "beauti", "pictur", "reckon", "time")
```

Next, I had to clean up the Corpus.  A Corpus is just a collection of text files, in this case, it is the collection of all the individual chapters from the six books.  I did this by making all the text lower case (`tolower` parameter, ensures that "Twain" doesn't show up as something from "twain"), removing punctuation (`removePunctuation` parameter), removing numbers (`removeNumbers` parameter), removing stop words (`stopwords` parameter), removing stemming (`stemming`, which reduces words of the same root to the same thing, ie "dog", "dog-like", "puppy" are all the same thing), and looking at words of three characters or more (`wordLengths`).  The last parameter, `weighting`, indicates that we are going to weight the importance of words by the standard term frequency (`tf`) method.    

```{r}
dtm.control = list(
  tolower = T,
  removePunctuation = T,
  removeNumbers = T,
  stopwords = c(stopwords("english"),extendedstopwords),
  stemming = T,
  wordLengths = c(3,Inf),
  weighting = weightTf)

dtm = DocumentTermMatrix(twaincorpus, control=dtm.control)
dtm = removeSparseTerms(dtm,0.999)
dtm = dtm[rowSums(as.matrix(dtm))>0,]
```

After cleaning the Corpus, we're ready to find our topics.  The last step is to decide how many topics we want `R` to find.  For this example, I decided to use 4 categories (k=4).  I went back and forth between this output and the extended stop words list to add necessary words I didn't want included in the topics.    

```{r}
k = 4
lda.model = LDA(dtm, k)
terms(lda.model,20)
```

Topic 1 looks like it is related to time of day and relationships with other people.  Topic 2 looks like it has to do with time of day again and speaking.  The third topic has to do with nature and the outdoors.  Lastly, topic 4 looks like it has to do with noblemen and religion.  To see what these topics look like in context, let's look at some randomly selected chapters that scored highly in each category.

```{r, include=FALSE}
twain.topics = posterior(lda.model, dtm)$topics
df.twain.topics = as.data.frame(twain.topics)
df.twain.topics = cbind(email=as.character(rownames(df.twain.topics)), 
                         df.twain.topics, stringsAsFactors=F)
```

```{r, echo = FALSE, eval=FALSE}
sample(which(df.twain.topics$"1" > .95), 10)
topic1 <- twaincorpus[[72]]
topic1$content[1:10]
```

```{r fig.width=2, fig.height=4,echo=FALSE}
library(png)
library(grid)
img <- readPNG("huckfinn35.png")
grid.raster(img)
```

This is one of the final chapters from \textit{The Adventures of Huckleberry Finn}.  This chapter describes the scene where Tom and Huck are planning their escape.  There is a lot of arguing going on because the boys are pressed for time and need a plan quick.  Topic 1 very loosely captures the essence of this chapter.

```{r, echo = FALSE, eval = FALSE}
sample(which(df.twain.topics$"2" > .95), 10)
topic2 <- twaincorpus[[10]]
topic2$content[1:10]
```

```{r fig.width=2, fig.height=4,echo=FALSE}
img <- readPNG("connyankee.png")
grid.raster(img)
```

This is a chapter from \textit{A Connecticut Yankee in King Arthur's Court}.  Here, an imprisoned man states that a king could go unrecognized in the streets if wearing peasant's clothing.  The king tests it out and finds that it is indeed true.  There is a lot of dialogue and inner stream of consciousness in this chapter, which again, is very loosely captured by Topic 2.

```{r, echo = FALSE, eval = FALSE}
sample(which(df.twain.topics$"3" > .95), 10)
topic3 <- twaincorpus[[208]]
topic3$content[1:10]
```

```{r fig.width=2, fig.height=4,echo=FALSE}
img <- readPNG("roughing.png")
grid.raster(img)
```

This excerpt is from \textit{Roughing It}.  This chapter describes a journey by mule, and they are currently somewhere in Nevada.  Topic 3 perfectly sums up this chapter, as it is full of flowery descriptions of nature and the natural world around them.

```{r, echo = FALSE, eval = FALSE}
sample(which(df.twain.topics$"4" > .95), 10)
topic4 <- twaincorpus[[132]]
topic4$content[1:10]
```

```{r fig.width=2, fig.height=4,echo=FALSE}
img <- readPNG("innocents.png")
grid.raster(img)
```

This is a chapter taken from \textit{Innocents Abroad}.  In this chapter, Twain wonders about Jesus's life as a boy because they are at Nazareth, Jesus's boyhood home.  Topic 4 also succintly describes the essence of this chapter. 




